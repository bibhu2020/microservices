import os
from flask_restful import Resource
from flask import request, jsonify
from datetime import datetime, timedelta

from azure.identity import AzureCliCredential # ManagedIdentityCredential #AzureCliCredential
from azure.monitor.query import LogsQueryClient

# Authenticate using Azure's default credentials
credential = AzureCliCredential() #ManagedIdentityCredential("8e4cd7a8-09ff-41a4-aa8b-e8843105756d")
client = LogsQueryClient(credential)

# Define your Application Insights workspace ID
workspace_id = 'edf08e1f-916f-48c3-bc52-273492d63c8f' #data connector

def query_application_insights(query):
    # Define the timespan as a tuple with a start datetime and a timedelta
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(minutes=15)  # Adjust the time span as needed
    timespan = (start_time, end_time)

    response = client.query_workspace(
        workspace_id,
        query,
        timespan=timespan
    )
    return response

failed_requests_query = """
AppRequests
| where AppRoleName contains "msonecloudapi" and Success != true
| order by TimeGenerated desc 
"""
# | project TimeGenerated, Name, ResultCode, OperationId

exceptions_query = """
AppExceptions
| where AppRoleName contains "msonecloudapi"
| project TimeGenerated, Type, Message, OperationId
| order by TimeGenerated desc 
"""

dependency_query = """
AppDependencies 
| where AppRoleName contains "msonecloudapi" and Success != true 
| project TimeGenerated, Target, Name, Data, OperationId 
| order by TimeGenerated desc 
"""

diagnostic_query = """
AzureDiagnostics 
| where toint(httpStatusCode_s) >= 400
| order by TimeGenerated desc 
"""

# Aggregate and analyze the data
import pandas as pd

def analyze_data0(diagnostics):
    try:
        # Convert LogsQueryResult to DataFrame
        def logs_query_result_to_df(logs_query_result):
            if logs_query_result.tables:
                table = logs_query_result.tables[0]
                columns = table.columns  # Ensure columns are correctly accessed
                rows = table.rows
                return pd.DataFrame(rows, columns=columns)
            return pd.DataFrame()

        # Convert to DataFrame
        failed_df = logs_query_result_to_df(diagnostics)
        
        # Debugging: Print the DataFrame columns and data
        print("Diagnostics Requests DataFrame Columns:", failed_df.columns)
        print("Diagnostics Requests DataFrame Data:", failed_df.head())

        # Check if DataFrames are empty
        if failed_df.empty:
            print("Diagnostics Requests DataFrame is empty.")

        # Take the top 5 rows for analysis
        top_data = failed_df.head(10)

        return top_data

    except Exception as e:
        print(f"An error occurred while analyzing data: {e}")
        return pd.DataFrame()  # Return an empty DataFrame in case of error
    

def analyze_data1(failed_requests, exceptions):
    try:
        # Convert LogsQueryResult to DataFrame
        def logs_query_result_to_df(logs_query_result):
            if logs_query_result.tables:
                table = logs_query_result.tables[0]
                columns = [col.name for col in table.columns]  # Ensure columns are correctly accessed
                rows = table.rows
                return pd.DataFrame(rows, columns=columns)
            return pd.DataFrame()

        # Convert to DataFrame
        failed_df = logs_query_result_to_df(failed_requests)
        exceptions_df = logs_query_result_to_df(exceptions)
        
        # Debugging: Print the DataFrame columns and data
        print("Failed Requests DataFrame Columns:", failed_df.columns)
        print("Failed Requests DataFrame Data:", failed_df.head())
        print("Exceptions DataFrame Columns:", exceptions_df.columns)
        print("Exceptions DataFrame Data:", exceptions_df.head())
        
        # Check if DataFrames are empty
        if failed_df.empty:
            print("Failed Requests DataFrame is empty.")
        if exceptions_df.empty:
            print("Exceptions DataFrame is empty.")
    
        # Merge DataFrames on operation_Id to find correlations
        merged_data = pd.merge(failed_df, exceptions_df, on='OperationId', how='left')

        # Check if merged DataFrame is empty
        if merged_data.empty:
            print("Merged DataFrame is empty.")

        # Take the top 5 rows for analysis
        top_data = merged_data.head(10)

        return top_data

    except Exception as e:
        print(f"An error occurred while analyzing data: {e}")
        return pd.DataFrame()  # Return an empty DataFrame in case of error

def analyze_data2(failed_requests, dependencies):
    try:
        # Convert LogsQueryResult to DataFrame
        def logs_query_result_to_df(logs_query_result):
            if logs_query_result.tables:
                table = logs_query_result.tables[0]
                columns = [col.name for col in table.columns]  # Ensure columns are correctly accessed
                rows = table.rows
                return pd.DataFrame(rows, columns=columns)
            return pd.DataFrame()

        # Convert to DataFrame
        failed_df = logs_query_result_to_df(failed_requests)
        dependencies_df = logs_query_result_to_df(dependencies)

        # Debugging: Print the DataFrame columns and data
        print("Failed Requests DataFrame Columns:", failed_df.columns)
        print("Failed Requests DataFrame Data:", failed_df.head())
        print("Dependencies DataFrame Columns:", dependencies_df.columns)
        print("Dependencies DataFrame Data:", dependencies_df.head()) 

        # Check if DataFrames are empty
        if failed_df.empty:
            print("Failed Requests DataFrame is empty.")
        if dependencies_df.empty:
            print("Dependencies DataFrame is empty.")

        # Merge DataFrames on operation_Id to find correlations
        merged_data = pd.merge(failed_df, dependencies_df, on='OperationId', how='left')

        # Check if merged DataFrame is empty
        if merged_data.empty:
            print("Merged DataFrame is empty.")

        # Take the top 5 rows for analysis
        top_data = merged_data.head(10)

        return top_data

    except Exception as e:
        print(f"An error occurred while analyzing data: {e}")
        return pd.DataFrame()  # Return an empty DataFrame in case of error

from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate

# Initialize OpenAI with your API key
llm = ChatOpenAI(temperature=0.5)

# Define the prompt template
prompt_template = PromptTemplate(
    input_variables=["context"],
    template="""
    Based on the following application data, provide detailed insights and pinpoint the root cause of any issues observed. Consider the following aspects:
    - Identify any patterns or anomalies in the data.
    - Correlate the data points to determine potential root causes.
    - Provide actionable recommendations to address the identified issues.
    - Specify which part of the system (e.g., specific service, endpoint, or operation) is likely causing the issue.
    - Highlight any specific error messages, failed operations, or unusual patterns that stand out.

    Application Data:
    {context}
    """
)

def generate_insights(merged_data):
    # Convert DataFrame to string for the prompt
    context = merged_data.to_string()

    # Use the LangChain model to generate insights
    prompt = prompt_template.format(context=context)
    response = llm(prompt)

    # Extract the text content from the response
    insights = response.content

    return insights

class AppInsightAPIV1(Resource):
    def get(self):
        try:
            # Query Application Insights
            print('Querying Application Insights...')
            failed_requests = query_application_insights(failed_requests_query)
            exceptions = query_application_insights(exceptions_query)
            dependencies = query_application_insights(dependency_query)
            diagnostics = query_application_insights(diagnostic_query)

            # Debugging: Print raw query results
            print("Failed Requests Query Result:", failed_requests)
            print("Exceptions Query Result:", exceptions)
            print("Dependencies Query Result:", dependencies)
            print("Diagnostics Query Result:", diagnostics)

            # Analyze the data
            print('Analyzing data...')
            analyzed_data0 = analyze_data0(diagnostics)

            # Debugging: Print analyzed data
            print("Analyzed Data:", analyzed_data0)

            # Generate insights
            print('Generating insights...')
            insights = generate_insights(analyzed_data0)
            print("Insights and Recommendations:")
            print(insights)

            # Analyze the data
            print('Analyzing data...')
            analyzed_data1 = analyze_data1(failed_requests, exceptions)

            # Debugging: Print analyzed data
            print("Analyzed Data:", analyzed_data1)

            # Generate insights
            print('Generating insights...')
            insights += generate_insights(analyzed_data1)
            print("Insights and Recommendations:")
            print(insights)

            # Analyze the data
            print('Analyzing data...')
            analyzed_data2 = analyze_data2(failed_requests, dependencies)

            # Debugging: Print analyzed data
            print("Analyzed Data:", analyzed_data2)

            if analyzed_data2.empty and analyzed_data1.empty:
                return {"result": "No anomaly observed in last 1 hour."}, 404

            # Generate insights
            print('Generating insights...')
            insights += generate_insights(analyzed_data2)
            print("Insights and Recommendations:")
            print(insights)
            
            # Return the result as JSON with the chat history
            return {"result": insights}, 200

        except Exception as e:
            return {"error": str(e)}, 500