import os
from langchain_openai import ChatOpenAI
from flask_restful import Resource
# from flask import request, jsonify
import logging

# Initialize the logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
handler = logging.StreamHandler()
handler.setLevel(logging.DEBUG)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

class TestAPIV1(Resource):
    def __init__(self):
        self._logger = logger
        self._llm = self._get_llm_client()

    def _get_llm_client(self):
        openai_api_key = os.environ.get("OPENAI_API_KEY")
        model_name = os.environ["OPENAI_API_MODEL"]
        if not openai_api_key:
            self._logger.error("OpenAI API key is not set properly.")
            raise EnvironmentError("OpenAI API key is not set properly.")
        llm = ChatOpenAI(api_key=openai_api_key, model=model_name, temperature=0.5)
        return llm
    
    def _test_chain(self):
        from langchain_core.output_parsers import StrOutputParser
        from langchain_core.prompts import ChatPromptTemplate
    
        # Define the first prompt to generate a joke
        prompt1 = ChatPromptTemplate.from_template("tell me a joke about {topic}")
    
        # Get a joke based on the topic provided by the user
        chain1 = prompt1 | self._llm | StrOutputParser()
        result1 = chain1.invoke({"topic": "chickens"})
    
        # Define the second prompt to analyze the joke
        prompt2 = ChatPromptTemplate.from_template("is this a funny joke? {joke}")
    
        # Analyze the joke generated by the model
        chain2 = prompt2 | self._llm | StrOutputParser()
        result2 = chain2.invoke({"joke": result1})
    
        # Define a lambda function to calculate the length of the joke
        length_lambda = lambda input: {"length": len(input)}
    
        # Calculate the response of chain2 using the lambda function as a runnable
        chain3 = chain2 | length_lambda
        result3 = chain3.invoke({"joke": result1})
    
        return {"status": "success", "joke": result1, "analysis": result2, "length": result3}

    def _test_chain2(self):
        from langchain_core.output_parsers import StrOutputParser
        from langchain_core.prompts import ChatPromptTemplate
        from langchain_core.runnables import RunnableParallel
    
        # Define the first prompt to generate a joke
        prompt1 = ChatPromptTemplate.from_template("tell me a joke about {topic}")
    
        # Get a joke based on the topic provided by the user
        chain1 = prompt1 | self._llm | StrOutputParser()
        result1 = chain1.invoke({"topic": "chickens"})
    
        # Define the second prompt to analyze the joke
        prompt2 = ChatPromptTemplate.from_template("is this a funny joke? {joke}")
    
        # # Analyze the joke generated by the model
        # chain2 = prompt2 | self._llm | StrOutputParser()
        # result2 = chain2.invoke({"joke": result1})
    
        # # Define a lambda function to calculate the length of the joke
        length_lambda = lambda input: {"length": len(input)}
    
        # # Calculate the response of chain2 using the lambda function as a runnable
        # chain3 = chain2 | length_lambda
        # result3 = chain3.invoke({"joke": result1})

        composed_chain_with_pipe = (
            RunnableParallel({"joke": chain1})
            .pipe(prompt2)
            .pipe(self._llm)
            .pipe(StrOutputParser())
            .pipe(length_lambda)
        )

        result2 = composed_chain_with_pipe.invoke({"topic": "battlestar galactica"})  
    
        return {"status": "success", "joke": result1, "analysis": result2}

    def get(self):
        try:
            # # Get the JSON data from the request
            # data = request.get_json()
            # if not data:
            #     self._logger.error("No JSON data found in the request.")
            #     return {"error": "No JSON data found in the request."}, 400

            # session_id = data.get('session_id')
            # text = data.get('text')

            # if not session_id or not text:
            #     self._logger.error("Missing 'session_id' or 'text' in the request.")
            #     return {"error": "Missing 'session_id' or 'text' in the request."}, 400

            # config = {"configurable": {"session_id": session_id}}

            # result = with_message_history.invoke(
            #     {"messages": [HumanMessage(content=text)], "language": "English"},
            #     config=config
            # )

            # retValue = {"status": "success", "data": result}

            retValue = self._test_chain2()

            # Return the result as JSON with the chat history
            return retValue, 200

        except Exception as e:
            self._logger.exception("An error occurred while processing the request.")
            return {"error": str(e)}, 500